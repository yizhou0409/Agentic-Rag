name: inference
exp_name: baseline

data:
  dataset_name: hotpotqa
  dataset_split: dev
  dataset_root_dir: ./data
  subset_size: null
  bon_traj_path: null # only for selecting bon trajectories

model: 
  reasoner_mode: offline
  reasoner_name_or_path: /scratch/yl9038/models/Qwen3-0.6B
  reasoner_url: http://127.0.0.1:30000
  summarizer_mode: offline
  summarizer_name_or_path: /scratch/yl9038/models/Qwen3-32B
  summarizer_url: http://127.0.0.1:30000
  tokenizer_path: /scratch/yl9038/models/Qwen3-0.6B

retrieval:
  host: 127.0.0.1
  port: 5123
  flashrag_config:
    retrieval_method: "bm25"
    index_path: /scratch/yl9038/longcot-rag-inference/indexes/bm25
    corpus_path: /scratch/yl9038/longcot-rag-inference/indexes/bm25/corpus.jsonl
    bm25_backend: "bm25s"
    retrieval_topk: 3
    retrieval_batch_size: 32
    retrieval_use_fp16: False
    retrieval_query_max_length: 128
    use_sentence_transformer: True
    faiss_gpu: true
    silent: false
    save_retrieval_cache: false
    use_retrieval_cache: false
    retrieval_cache_path: null
    use_reranker: false


train:
  batch_size: 32
  num_epochs: 10
  lr: 0.0001
  weight_decay: 0.0001
  num_workers: 4

eval:
  batch_size: 32
  num_workers: 4

inference:
  prompt_templates_dir: prompts
  user_message_template_name: default_QA
  max_turns: 10
  server_params:
    log_level: error
    tp_size: 1
    dp_size: 1
    mem_fraction_static: 0.75
    schedule_conservativeness: 0.3
    schedule_policy: fcfs
    # Quantization settings to save GPU memory
    # Using transformers+bitsandbytes fallback for reliable int8 quantization
    use_transformers_fallback: true
    quantization: int8  # BitsAndBytes int8 quantization
    #
    # Alternative SGLang options (if you want to try them):
    # use_transformers_fallback: false
    # quantization: bitsandbytes  # SGLang bitsandbytes
    # quantization: fp8  # FP8 quantization
    # quantization: w8a8_int8  # W8A8 int8 quantization
    #
    # For blockwise_int8 (requires additional config):
    # quantization: blockwise_int8
    # weight_block_size: 128  # Required for blockwise_int8
    #
    # For maximum memory savings:
    # quantization: int4  # BitsAndBytes int4 quantization
  reasoner_sampling_params:
    max_new_tokens: 2048
    temperature: 0.6
    top_p: 0.95
    repetition_penalty: 1.0
    no_stop_trim: True
  summarizer_sampling_params:
    max_new_tokens: 2048
    temperature: 0.6
    top_p: 0.95
    repetition_penalty: 1.0


hydra:
  job_logging:
    root:
      level: INFO
      handlers: [console, file]
    handlers:
      console:
        class: logging.StreamHandler
        stream: ext://sys.stdout
        formatter: simple
      file:
        class: logging.FileHandler
        filename: outputs/${hydra.job.name}/${data.dataset_name}/${now:%m-%d-%H-%M-%S}.log
        formatter: simple
        mode: a
    formatters:
      simple:
        format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
  run:
    dir: outputs/${hydra.job.name}/${data.dataset_name}/${now:%m-%d-%H-%M-%S}_${exp_name}
